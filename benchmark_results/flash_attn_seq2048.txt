==== Attention Benchmark ====
Config: batch=2, seqLen=2048, contextLen=2048, heads=8, dim=64

Flash Attention:
  Time: 3856.98 ms per run
  Performance: 4.54123 GFLOPs/sec

Standard Attention:
  Time: 5426.64 ms per run
  Performance: 3.22767 GFLOPs/sec

Speedup: 1.40697x

Correctness Check:
  Max difference: 5.82695e-05
  Average difference: 1.32302e-06
  Elements with diff > 1e-4: 0 (0%)
