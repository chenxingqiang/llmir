==== Attention Benchmark ====
Config: batch=2, seqLen=512, contextLen=512, heads=8, dim=64

Flash Attention:
  Time: 313.748 ms per run
  Performance: 3.48915 GFLOPs/sec

Standard Attention:
  Time: 464.847 ms per run
  Performance: 2.355 GFLOPs/sec

Speedup: 1.48159x

Correctness Check:
  Max difference: 6.21527e-05
  Average difference: 1.48081e-06
  Elements with diff > 1e-4: 0 (0%)
