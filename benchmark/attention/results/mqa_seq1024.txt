==== Multi-Query Attention Benchmark ====
Config: batch=2, seqLen=1024, contextLen=1024, heads=12, dim=64

Standard Multi-Head Attention:
  Time: 2009.07 ms per run
  Performance: 3.20668 GFLOPs/sec
  Memory: 24 MB

Multi-Query Attention:
  Time: 2802.46 ms per run
  Performance: 1.14943 GFLOPs/sec
  Memory: 13 MB
  Memory Reduction: 45.8333%

Speedup: 0.716897x

Output Difference (MHA vs MQA):
  Max Difference: 0.127874
  Average Difference: 0.00269591
