==== Attention Benchmark ====
Config: batch=2, seqLen=1024, contextLen=1024, heads=8, dim=64

Flash Attention:
  Time: 967.483 ms per run
  Performance: 4.52603 GFLOPs/sec

Standard Attention:
  Time: 1429.23 ms per run
  Performance: 3.06377 GFLOPs/sec

Speedup: 1.47727x

Correctness Check:
  Max difference: 4.77561e-05
  Average difference: 1.54645e-06
  Elements with diff > 1e-4: 0 (0%)
